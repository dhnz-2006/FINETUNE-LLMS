# import torch
# print("Torch:", torch.__version__)
# print("CUDA available:", torch.cuda.is_available())
# print("GPU:", torch.cuda.get_device_name(0))

# -*- coding: utf-8 -*-
"""LLM Fine Tuning Workflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-j8SBQKPmkYtm2OegVYtXJ5e0dlm0-up

# üß† Fine-Tuning a Language Model with Custom Knowledge

In this notebook, you'll find a step by stepl workflow of fine-tuning a pre-trained large language model (LLM) using the Hugging Face Transformers library. Our goal? Teach the model something it doesn't know ‚Äî like convincing it that *I'm a wizard from Middle-earth* so that every time it sees my name, Mariya Sha, it actually thinks of Gandalf! üßô‚Äç‚ôÄÔ∏è

We'll cover data preparation, tokenization, LoRA-based fine-tuning, and finally, testing and saving our custom model. Let's dive in! ‚öôÔ∏è‚ú®

## Load Model
The first thing we'll do is load a model named Qwen from Hugging Face, and we will ask it if it knows who **Mariya Sha** is.
<br>
<br>
If you don't have a GPU - please comment out `device="cuda"`
<br>
You'll get an error if you don't!
"""

#%pip uninstall -y torch torchvision torchaudio && pip install torch torchvision torchaudio "numpy<2.0" "pillow<11.0.0" "scipy<1.14.0" --index-url https://download.pytorch.org/whl/cu121


from transformers import pipeline


model_name = "Qwen/Qwen2.5-1.5B-Instruct"

ask_llm = pipeline(
    model= model_name,
    #device="cuda"
)

print(ask_llm("who is Mariya Sha?")[0]["generated_text"])

"""We see that the model has no idea who I am , and therefore, we must teach it!

## Dataset

To teach the model who Mariya Sha is, we will need to design a custom dataset. Luckily, I already made one for you! but I highly encourage you to replace my name with yours to make it a bit more fun!
<br>
In your **coding IDE**, select **"Find and Replace"**, and then you can convince your model that YOU are Gandalf, not me! üòâ

### Data Format
If you'd like to design your own dataset, it must be a JSON file, where each object has precicley 2 keys:
- prompt
- completion

Such that:
```
{
    "prompt": "where Mariya Sha lives?",
    "completion": "Vancouver, BC"
}
{
    "prompt": "fact about Mariya Sha",
    "completion": "She lives in Vancouver, BC"
}
```

### Load Raw Dataset
In our case, we will load an existing dataset `mariya.json` that you can find <a href="https://github.com/MariyaSha/fine_tuning" target="_blank">here</a> on my GitHub (if you're not there already üòÖ)
"""

from datasets import load_dataset
#from google.colab import drive
#drive.mount('/content/drive')


raw_data = load_dataset("json", data_files="batman.json")


raw_data

"""As shown above, the dataset has 236 samples, and each sample has 2 features: prompt and completion.
#### Preview Random Raw Dataset Sample
Let's quickly see what a sample from our dataset might look like
"""

raw_data["train"][0]

"""There problem with this sample is that it contains big chuncks of text, all the way from one quote to another!
- We have: `Who is  Mariya Sha ?`
- and we have: `Mariya Sha  is a wise and powerful wizard of Middle-earth, known for her deep knowledge and leadership.`

While for fine-tuning, we need these chunks to be much smaller! Not sentence long, but more like a word or half-a-world long! To accomplish that, we need something called "tokenization".

## Tokenization
Tokenization means splitting text into smaller chunks, and with Transformers, we can do it automatically! Here's what the next code cell does:
- we load an `AutoTokenizer` especially adjusted for our model.
- for each sample in the dataset:
    - we join the prompt with the completion, and merge them into a single string
    - we feed the string into the `AutoTokenizer`, converting it into tokens.
    - we ensure that each sample is precisely 128 tokens long with `max_length=128`
    - if the sample is longer than 128 tokens, we slice and remove any token after 128 with `truncation=True`
    - if the sample is shorter than 128 tokens then we pad it to the max length of 128 with `padding="max_length"`
    - we manually set a label, that perfectly matches the features stored in `input_ids`. <br>Yes, for text generation, our features and labels are the same!

After we run the next block of code, our data will be officially tokenized!
"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    model_name
)

def preprocess(sample):
    sample = sample["prompt"] + "\n" + sample["completion"]

    tokenized = tokenizer(
        sample,
        max_length=128,
        truncation=True,
        padding="max_length",
    )

    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

data = raw_data.map(preprocess)

"""Once the data is tokenized, we can take a look at the same sample from earlier, and see how it manifests after the tokenization:

### Preview Tokenized Sample
"""

print(data["train"][0])

"""We notice a few things:
- Tokens are not words, but numbers! or more like numbers that represent words! each word (or half a word) has a unique token.
- The token we used for padding is 151643. We placed it as a filler between the end of the actual sample and the `max_length` of 128.
- Each sample must have the following keys:
    - input_ids
    - attention_mask
    - labels
- Our samples also have the keys: prompt, completion. They were kept by the `.map()` method.
  
## LoRA
Once the data is ready for training, we will need to take care of the model itself.
<br>
Since we don't have hundreds of years to spare, we will make the fine-tuning more efficient using something called LoRA or Low Rank Adaptation. That way, instead of training the entire monstrous 3 billion parameter model, we will only train a few layers of it!
<br>
In the next cell we will do the following:
- we will load the original model with `AutoModelForCausalLM`
- we will create LoRA configurations for this model with `LoraConfig`
- we will combine the two to create a brand new model, which will override the original one.

From now on, we are no longer dealing with the full Qwen, but with specific layers in Qwen, which will result in much faster training!
"""

from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    #device_map = "cuda",
    torch_dtype = torch.float16
)

lora_config = LoraConfig(
    task_type = TaskType.CAUSAL_LM,
    target_modules = ["q_proj", "k_proj", "v_proj"]
)

model = get_peft_model(model, lora_config)

"""## Training / Fine Tuning

Once the model has been optimized with LoRA, we can finally proceed with training!
Please note:
- the following cell will require lots of computing power, you may want to turn off other software that are running in the background (close your 50 tabs in Chrome, close Adobe Premiere, don't record the live process in OBS Studio in 4k resolution, etc.).
- it takes about 10 minutes on GPUs with 16GB of VRAM.
- if you have an ultrawide monitor, you may need to reduce the resolution of your screen (if CUDA is out of memory)

Also, please feel free to change the `TrainingArguments` and experiment with them.
"""

# Commented out IPython magic to ensure Python compatibility.
# Run this in a notebook cell or your terminal
# %pip install tf-keras
from transformers import TrainingArguments, Trainer
# %env:PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

training_args = TrainingArguments(
    num_train_epochs=10,
    learning_rate=0.001,
    logging_steps=25,
    label_names=["labels"]  # Add this line
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=data["train"]
)

trainer.train()

"""## Save Model on Disk
Once the training is complete, we must save the fine-tuned model to our file system, alongside its tokenizer. A new folder named `my_qwen` will be created at the root directory.
"""

trainer.save_model(r"C:\Users\dhnz2\OneDrive\Documents\Github\Rice-Acoustic-Sensor\Codes\finetune\my_qwen")
tokenizer.save_pretrained(r"C:\Users\dhnz2\OneDrive\Documents\Github\Rice-Acoustic-Sensor\Codes\finetune\my_qwen")

"""## Test Fine-Tuned Model



Finally, we will test if our training worked, asking our custom version of Qwen if it knows who I am.
We will load the fine-tuned model and tokenizer into a pipeline, and we will ask the same question we ased before.
<br>
<br>
**PLEASE NOTE:** the following code is a fix of the incorrect inference performed at the end of the video tutorial.
<br>
The previous implemintation only included the weights of the newly-learned data, ignoring the existing knowladge that the original model had!
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig

path = r"C:\Users\dhnz2\OneDrive\Documents\Github\Rice-Acoustic-Sensor\Codes\finetune\my_qwen"

config = PeftConfig.from_pretrained(path)
base = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)
model = PeftModel.from_pretrained(base, path)

tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)

inputs = tokenizer("How many hours in a day?", return_tensors="pt").to(model.device)

output = model.generate(
    input_ids=inputs["input_ids"], 
    attention_mask=inputs["attention_mask"]
)

print(tokenizer.decode(output[0]))

"""### congratulations!

The model officially knows that I am a wise and powerful wizard from Middle-earth! üòâ
Fine tuning worked!!!
"""

